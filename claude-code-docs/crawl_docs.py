#!/usr/bin/env python3
"""
Claude Code æ–‡æ¡£çˆ¬è™«è„šæœ¬
å°† https://code.claude.com/docs çš„æ–‡æ¡£çˆ¬å–å¹¶è½¬æ¢ä¸º Markdown æ–‡ä»¶
"""

import os
import re
import time
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from markdownify import markdownify as md

# é…ç½®
BASE_URL = "https://code.claude.com/docs"
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(SCRIPT_DIR, "docs")
LANGUAGE = "en"  # çˆ¬å–è‹±æ–‡æ–‡æ¡£

# æ–‡æ¡£é¡µé¢åˆ—è¡¨
DOC_PAGES = [
    # Getting started
    "overview",
    "quickstart",
    "common-workflows",
    "changelog",
    # Outside of the terminal
    "claude-code-on-the-web",
    "desktop",
    "chrome",
    "vs-code",
    "jetbrains",
    "github-actions",
    "gitlab-ci-cd",
    "slack",
    # Build with Claude Code
    "sub-agents",
    "plugins",
    "discover-plugins",
    "skills",
    "output-styles",
    "hooks-guide",
    "headless",
    "mcp",
    "troubleshooting",
    # Deployment
    "third-party-integrations",
    "amazon-bedrock",
    "google-vertex-ai",
    "microsoft-foundry",
    "network-config",
    "llm-gateway",
    "devcontainer",
    "sandboxing",
    # Administration
    "setup",
    "iam",
    "security",
    "data-usage",
    "monitoring-usage",
    "costs",
    "analytics",
    "plugin-marketplaces",
    # Configuration
    "settings",
    "terminal-config",
    "model-config",
    "memory",
    "statusline",
    # Reference
    "cli-reference",
    "interactive-mode",
    "slash-commands",
    "checkpointing",
    "hooks",
    "plugins-reference",
    # Resources
    "legal-and-compliance",
]

# HTTP è¯·æ±‚å¤´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
}


def fetch_page(url: str) -> str | None:
    """è·å–é¡µé¢ HTML å†…å®¹"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        print(f"  âŒ è·å–å¤±è´¥: {e}")
        return None


def extract_main_content(html: str) -> tuple[str, str]:
    """ä» HTML ä¸­æå–ä¸»è¦å†…å®¹åŒºåŸŸï¼Œè¿”å› (title, content_html)"""
    soup = BeautifulSoup(html, "html.parser")

    # æå–æ ‡é¢˜
    title = ""
    title_tag = soup.find("h1")
    if title_tag:
        title = title_tag.get_text(strip=True)

    # å°è¯•æ‰¾åˆ°ä¸»å†…å®¹åŒºåŸŸ - é€šå¸¸åœ¨ article æˆ– main æ ‡ç­¾ä¸­
    main_content = None

    # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨
    selectors = [
        "article",
        "main",
        "[role='main']",
        ".docs-content",
        ".content",
        "#content",
        ".markdown-body",
    ]

    for selector in selectors:
        main_content = soup.select_one(selector)
        if main_content:
            break

    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨ body
    if not main_content:
        main_content = soup.find("body")

    if main_content:
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in main_content.select("nav, header, footer, script, style, .sidebar, .toc, .navigation"):
            element.decompose()

        return title, str(main_content)

    return title, ""


def html_to_markdown(html: str, base_url: str) -> str:
    """å°† HTML è½¬æ¢ä¸º Markdown"""
    # å…ˆç”¨ BeautifulSoup æ¸…ç† HTML
    soup = BeautifulSoup(html, "html.parser")

    # ç§»é™¤æŒ‰é’®å’Œäº¤äº’å…ƒç´ 
    for btn in soup.select("button, .copy-button, [data-copy], .ask-ai"):
        btn.decompose()

    # ç§»é™¤ "Copy" å’Œ "Ask AI" æ–‡æœ¬èŠ‚ç‚¹
    for text_node in soup.find_all(string=re.compile(r"^(Copy|Ask AI)$")):
        if text_node.parent and text_node.parent.name not in ["code", "pre"]:
            text_node.extract()

    # ä¿®å¤ä»£ç å—è¯­è¨€
    for code in soup.select("code"):
        classes = code.get("class", [])
        for cls in classes:
            if "language-shiki" in cls or cls == "shiki":
                # å°è¯•ä»çˆ¶å…ƒç´ æˆ– data å±æ€§è·å–çœŸå®è¯­è¨€
                code["class"] = [c.replace("shiki", "bash") for c in classes]

    html = str(soup)

    # ä½¿ç”¨ markdownify è½¬æ¢
    markdown = md(
        html,
        heading_style="ATX",
        bullets="-",
        code_language_callback=lambda el: el.get("class", [""])[0].replace("language-", "") if el.get("class") else "",
    )

    # æ¸…ç† shiki è¯­è¨€æ ‡è®°
    markdown = re.sub(r"```shiki\b", "```bash", markdown)
    markdown = re.sub(r"```language-shiki\b", "```bash", markdown)

    # ç§»é™¤æ®‹ç•™çš„ "Copy" å’Œ "Ask AI"
    markdown = re.sub(r"\nCopy\n", "\n", markdown)
    markdown = re.sub(r"\nAsk AI\n", "\n", markdown)
    markdown = re.sub(r"^Copy$", "", markdown, flags=re.MULTILINE)
    markdown = re.sub(r"^Ask AI$", "", markdown, flags=re.MULTILINE)

    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
    markdown = re.sub(r"\n{3,}", "\n\n", markdown)

    # æ¸…ç†è¡Œé¦–è¡Œå°¾ç©ºç™½
    lines = [line.rstrip() for line in markdown.split("\n")]
    markdown = "\n".join(lines)

    return markdown.strip()


def save_markdown(content: str, filename: str):
    """ä¿å­˜ Markdown æ–‡ä»¶"""
    filepath = os.path.join(OUTPUT_DIR, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"  âœ… å·²ä¿å­˜: {filename}")


def crawl_page(page_slug: str) -> bool:
    """çˆ¬å–å•ä¸ªé¡µé¢"""
    url = f"{BASE_URL}/{LANGUAGE}/{page_slug}"
    print(f"\nğŸ“„ æ­£åœ¨çˆ¬å–: {page_slug}")
    print(f"   URL: {url}")

    html = fetch_page(url)
    if not html:
        return False

    title, content_html = extract_main_content(html)
    if not content_html:
        print("  âš ï¸ æœªæ‰¾åˆ°å†…å®¹")
        return False

    markdown = html_to_markdown(content_html, url)

    # æ·»åŠ æ–‡æ¡£å¤´ä¿¡æ¯
    header = f"""---
source: {url}
title: {title}
---

"""

    full_content = header + markdown

    # ä¿å­˜æ–‡ä»¶
    filename = f"{page_slug}.md"
    save_markdown(full_content, filename)

    return True


def create_index(successful_pages: list[str]):
    """åˆ›å»ºç´¢å¼•æ–‡ä»¶"""
    index_content = """---
title: Claude Code Documentation Index
---

# Claude Code æ–‡æ¡£ç´¢å¼•

æ­¤æ–‡ä»¶å¤¹åŒ…å«ä» https://code.claude.com/docs çˆ¬å–çš„æ–‡æ¡£ã€‚

## æ–‡æ¡£åˆ—è¡¨

"""

    categories = {
        "Getting Started": ["overview", "quickstart", "common-workflows", "changelog"],
        "Outside of the Terminal": ["claude-code-on-the-web", "desktop", "chrome", "vs-code", "jetbrains", "github-actions", "gitlab-ci-cd", "slack"],
        "Build with Claude Code": ["sub-agents", "plugins", "discover-plugins", "skills", "output-styles", "hooks-guide", "headless", "mcp", "troubleshooting"],
        "Deployment": ["third-party-integrations", "amazon-bedrock", "google-vertex-ai", "microsoft-foundry", "network-config", "llm-gateway", "devcontainer", "sandboxing"],
        "Administration": ["setup", "iam", "security", "data-usage", "monitoring-usage", "costs", "analytics", "plugin-marketplaces"],
        "Configuration": ["settings", "terminal-config", "model-config", "memory", "statusline"],
        "Reference": ["cli-reference", "interactive-mode", "slash-commands", "checkpointing", "hooks", "plugins-reference"],
        "Resources": ["legal-and-compliance"],
    }

    for category, pages in categories.items():
        index_content += f"### {category}\n\n"
        for page in pages:
            if page in successful_pages:
                index_content += f"- [{page}](./{page}.md)\n"
        index_content += "\n"

    save_markdown(index_content, "INDEX.md")


def main():
    """ä¸»å‡½æ•°"""
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    print("=" * 60)
    print("Claude Code æ–‡æ¡£çˆ¬è™«")
    print("=" * 60)
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"æ–‡æ¡£æ•°é‡: {len(DOC_PAGES)}")

    successful = []
    failed = []

    for page_slug in DOC_PAGES:
        if crawl_page(page_slug):
            successful.append(page_slug)
        else:
            failed.append(page_slug)

        # ç¤¼è²Œæ€§å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(0.5)

    # åˆ›å»ºç´¢å¼•
    print("\nğŸ“‘ åˆ›å»ºç´¢å¼•æ–‡ä»¶...")
    create_index(successful)

    # æ‰“å°ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("çˆ¬å–å®Œæˆ!")
    print(f"æˆåŠŸ: {len(successful)}/{len(DOC_PAGES)}")

    if failed:
        print(f"å¤±è´¥: {failed}")

    print("=" * 60)


if __name__ == "__main__":
    main()
